#!/usr/bin/env python3
"""
í´ëŸ¬ìŠ¤í„°ë§ ê¸°ë°˜ ë°°ì°¨ ìµœì í™” ì•Œê³ ë¦¬ì¦˜

ê²½ë¡œ ì¤‘ë³µ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ê³ ê¸‰ í´ëŸ¬ìŠ¤í„°ë§ ë° ë°°ì°¨ ìµœì í™”
src/algorithm ëª¨ë“ˆë¡œ í†µí•©ë¨
"""

import json
import math
import numpy as np
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Tuple
from sklearn.cluster import DBSCAN, KMeans
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from concurrent.futures import ThreadPoolExecutor
import multiprocessing
from src.model.delivery_point import DeliveryPoint
from src.model.vehicle import Vehicle
from src.utils.distance_calculator import calculate_distance, calculate_distances_batch
import logging

logger = logging.getLogger(__name__)

class ClusterOptimizer:
    """í´ëŸ¬ìŠ¤í„°ë§ ê¸°ë°˜ ë°°ì°¨ ìµœì í™”ê¸° - DBSCAN + Balanced K-means"""
    
    def __init__(self, data: Dict = None, data_file: str = None):
        """
        ì´ˆê¸°í™”
        Args:
            data: ì§ì ‘ ì „ë‹¬ëœ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
            data_file: ë°ì´í„° íŒŒì¼ ê²½ë¡œ (data ì—†ì„ ë•Œë§Œ ì‚¬ìš©)
        """
        if data is not None:
            self.data = data
        elif data_file is not None:
            with open(data_file, 'r', encoding='utf-8') as f:
                self.data = json.load(f)
        else:
            raise ValueError("data ë˜ëŠ” data_file ì¤‘ í•˜ë‚˜ëŠ” ë°˜ë“œì‹œ ì œê³µë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")
        
        self.routes = self.data['routes']
        self.tc_stats = self.data.get('tc_stats', [])
        
        # ì„±ëŠ¥ ìµœì í™” ì„¤ì •
        self.max_workers = min(4, multiprocessing.cpu_count())
        self.max_iterations = self.max_workers * 2  # 8íšŒë¡œ ê°ì†Œ
        self.sample_size = min(100, len(self.routes) // 2)  # ìƒ˜í”Œ í¬ê¸° ê°ì†Œ
        self.distance_cache = {}  # ê±°ë¦¬ ê³„ì‚° ìºì‹±
        
        print(f"ğŸ”§ í´ëŸ¬ìŠ¤í„° ìµœì í™”ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   - ìµœëŒ€ ë°˜ë³µ: {self.max_iterations}íšŒ")
        print(f"   - ìƒ˜í”Œ í¬ê¸°: {self.sample_size}ê°œ")
        print(f"   - ë³‘ë ¬ ì›Œì»¤: {self.max_workers}ê°œ")
        
    def calculate_distance(self, lat1: float, lng1: float, lat2: float, lng2: float) -> float:
        """ë‘ ì¢Œí‘œ ê°„ ê±°ë¦¬ ê³„ì‚° (km) - ìºì‹œ ì ìš©"""
        # ìºì‹œ í‚¤ ìƒì„± (ì†Œìˆ˜ì  4ìë¦¬ë¡œ ë°˜ì˜¬ë¦¼í•˜ì—¬ ìºì‹œ íš¨ìœ¨ì„± ì¦ëŒ€)
        key = (round(lat1, 4), round(lng1, 4), round(lat2, 4), round(lng2, 4))
        
        if key in self.distance_cache:
            return self.distance_cache[key]
        
        # í†µí•©ëœ ê±°ë¦¬ ê³„ì‚° í•¨ìˆ˜ ì‚¬ìš©
        distance = calculate_distance(lat1, lng1, lat2, lng2)
        self.distance_cache[key] = distance
        return distance
    
    def extract_delivery_points_by_tc(self) -> Dict:
        """TCë³„ ë°°ì†¡ì§€ ì¶”ì¶œ - ìµœì í™”"""
        tc_deliveries = defaultdict(list)
        
        for route in self.routes:
            tc_id = route.get('depot_id', 'unknown')
            delivery_coords = [coord for coord in route['coordinates'] if coord['type'] == 'delivery']
            
            for coord in delivery_coords:
                tc_deliveries[tc_id].append({
                    'lat': coord['lat'],
                    'lng': coord['lng'],
                    'label': coord['label'],
                    'current_vehicle': route['vehicle_id']
                })
        
        return tc_deliveries
    
    def optimize_tc_clustering(self, tc_id: str, deliveries: List[Dict], vehicle_count: int) -> List[List[Dict]]:
        """TCë³„ ìµœì í™”ëœ í´ëŸ¬ìŠ¤í„°ë§ - ì„±ëŠ¥ ìµœì í™”"""
        print(f"\nğŸ”§ {tc_id} í´ëŸ¬ìŠ¤í„°ë§ ìµœì í™” ì¤‘...")
        
        if len(deliveries) < vehicle_count:
            print(f"  âš ï¸ ë°°ì†¡ì§€ ìˆ˜({len(deliveries)})ê°€ ì°¨ëŸ‰ ìˆ˜({vehicle_count})ë³´ë‹¤ ì ìŒ")
            return [deliveries]
        
        # ì¢Œí‘œ ë°ì´í„° ì¤€ë¹„
        coords = np.array([[d['lat'], d['lng']] for d in deliveries])
        
        # 1. ê°„ì†Œí™”ëœ DBSCAN (ìƒ˜í”Œë§ ê¸°ë°˜)
        sample_size = min(100, len(coords))
        sample_indices = np.random.choice(len(coords), sample_size, replace=False)
        sample_coords = coords[sample_indices]
        
        scaler = StandardScaler()
        sample_coords_scaled = scaler.fit_transform(sample_coords)
        
        # eps ê°’ì„ ë™ì ìœ¼ë¡œ ê³„ì‚° (ìƒ˜í”Œ ê¸°ë°˜)
        distances = []
        for i in range(min(50, len(sample_coords))):
            for j in range(i+1, min(50, len(sample_coords))):
                dist = self.calculate_distance(sample_coords[i][0], sample_coords[i][1], 
                                             sample_coords[j][0], sample_coords[j][1])
                distances.append(dist)
        
        avg_distance = np.mean(distances) if distances else 5.0
        eps = avg_distance / 3
        
        dbscan = DBSCAN(eps=eps/100, min_samples=max(2, sample_size//vehicle_count//2))
        sample_labels = dbscan.fit_predict(sample_coords_scaled)
        
        # 2. K-meansë¡œ ì§ì ‘ í´ëŸ¬ìŠ¤í„°ë§ (DBSCAN ê²°ê³¼ ë¬´ì‹œí•˜ê³  ë‹¨ìˆœí™”)
        kmeans = KMeans(
            n_clusters=vehicle_count,
            init='k-means++',
            n_init=5,  # ë°˜ë³µ íšŸìˆ˜ ê°ì†Œ
            max_iter=100,  # ìµœëŒ€ ë°˜ë³µ ê°ì†Œ
            random_state=42
        )
        
        cluster_labels = kmeans.fit_predict(coords)
        
        # 3. ê°„ì†Œí™”ëœ í´ëŸ¬ìŠ¤í„° ê· í˜• ì¡°ì •
        cluster_labels = self.balance_clusters_fast(coords, cluster_labels, vehicle_count)
        
        # 4. í´ëŸ¬ìŠ¤í„°ë³„ ë°°ì†¡ì§€ ê·¸ë£¹í™”
        clusters = [[] for _ in range(vehicle_count)]
        for i, label in enumerate(cluster_labels):
            clusters[label].append(deliveries[i])
        
        # 5. ë¹ˆ í´ëŸ¬ìŠ¤í„° ì²˜ë¦¬
        clusters = [cluster for cluster in clusters if cluster]
        
        # 6. ê°„ì†Œí™”ëœ í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ í‰ê°€
        self.evaluate_clustering_quality_fast(clusters, tc_id)
        
        return clusters
    
    def balance_clusters_fast(self, coords: np.ndarray, labels: np.ndarray, n_clusters: int) -> np.ndarray:
        """í´ëŸ¬ìŠ¤í„° í¬ê¸° ê· í˜• ì¡°ì • - ê³ ì† ë²„ì „"""
        target_size = len(coords) // n_clusters
        tolerance = max(5, target_size // 3)  # í—ˆìš© ì˜¤ì°¨ ì¦ê°€
        
        # í˜„ì¬ í´ëŸ¬ìŠ¤í„° í¬ê¸° ê³„ì‚°
        cluster_sizes = [np.sum(labels == i) for i in range(n_clusters)]
        
        # í¬ê¸°ê°€ ë¶ˆê· í˜•í•œ ê²½ìš° ì¬ì¡°ì • (ë°˜ë³µ íšŸìˆ˜ ê°ì†Œ)
        max_iterations = 5  # ê¸°ì¡´ 10ì—ì„œ ê°ì†Œ
        for iteration in range(max_iterations):
            rebalanced = False
            
            for i in range(n_clusters):
                if cluster_sizes[i] > target_size + tolerance:
                    cluster_points = np.where(labels == i)[0]
                    
                    # ê°€ì¥ ì‘ì€ í´ëŸ¬ìŠ¤í„° ì°¾ê¸°
                    min_cluster = np.argmin(cluster_sizes)
                    if cluster_sizes[min_cluster] < target_size - tolerance:
                        # ê°„ë‹¨í•œ ì´ë™ (ê±°ë¦¬ ê³„ì‚° ìƒëµ)
                        if len(cluster_points) > 0:
                            # ëœë¤í•˜ê²Œ í•˜ë‚˜ ì„ íƒ (ê±°ë¦¬ ê³„ì‚° ìƒëµìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ)
                            random_idx = np.random.choice(cluster_points)
                            labels[random_idx] = min_cluster
                            
                            cluster_sizes[i] -= 1
                            cluster_sizes[min_cluster] += 1
                            rebalanced = True
            
            if not rebalanced:
                break
        
        return labels
    
    def evaluate_clustering_quality_fast(self, clusters: List[List[Dict]], tc_id: str) -> None:
        """í´ëŸ¬ìŠ¤í„°ë§ í’ˆì§ˆ í‰ê°€ - ê³ ì† ë²„ì „"""
        print(f"  ğŸ“Š {tc_id} í´ëŸ¬ìŠ¤í„°ë§ í’ˆì§ˆ í‰ê°€:")
        
        total_intra_distance = 0
        
        # ê°„ì†Œí™”ëœ í’ˆì§ˆ í‰ê°€ (ìƒ˜í”Œë§ ê¸°ë°˜)
        for i, cluster in enumerate(clusters):
            if len(cluster) < 2:
                print(f"    ğŸš› í´ëŸ¬ìŠ¤í„° {i+1}: {len(cluster)}ê°œ ë°°ì†¡ì§€ (ë‹¨ì¼ ì§€ì )")
                continue
                
            # ìƒ˜í”Œë§ìœ¼ë¡œ ê±°ë¦¬ ê³„ì‚° (ìµœëŒ€ 5ê°œ ìƒ˜í”Œ)
            sample_size = min(5, len(cluster))
            sample_indices = np.random.choice(len(cluster), sample_size, replace=False)
            
            cluster_distances = []
            for j in range(len(sample_indices)):
                for k in range(j+1, len(sample_indices)):
                    idx_j, idx_k = sample_indices[j], sample_indices[k]
                    dist = self.calculate_distance(
                        cluster[idx_j]['lat'], cluster[idx_j]['lng'],
                        cluster[idx_k]['lat'], cluster[idx_k]['lng']
                    )
                    cluster_distances.append(dist)
            
            if cluster_distances:
                avg_intra = np.mean(cluster_distances)
                total_intra_distance += avg_intra
                print(f"    ğŸš› í´ëŸ¬ìŠ¤í„° {i+1}: {len(cluster)}ê°œ ë°°ì†¡ì§€, í‰ê·  ë‚´ë¶€ ê±°ë¦¬ {avg_intra:.2f}km")
        
        # í´ëŸ¬ìŠ¤í„° ê°„ ê±°ë¦¬ (ìƒ˜í”Œë§)
        cluster_centers = []
        for cluster in clusters:
            if cluster:
                center_lat = np.mean([d['lat'] for d in cluster])
                center_lng = np.mean([d['lng'] for d in cluster])
                cluster_centers.append((center_lat, center_lng))
        
        if len(cluster_centers) > 1:
            # ìµœëŒ€ 3ê°œ ìƒ˜í”Œë§Œ ê³„ì‚°
            sample_size = min(3, len(cluster_centers))
            inter_distances = []
            for i in range(sample_size):
                for j in range(i+1, len(cluster_centers)):
                    dist = self.calculate_distance(
                        cluster_centers[i][0], cluster_centers[i][1],
                        cluster_centers[j][0], cluster_centers[j][1]
                    )
                    inter_distances.append(dist)
            
            if inter_distances:
                avg_inter = np.mean(inter_distances)
                print(f"    ğŸ“ í‰ê·  í´ëŸ¬ìŠ¤í„° ê°„ ê±°ë¦¬: {avg_inter:.2f}km")
                
                # ì‹¤ë£¨ì—£ ì ìˆ˜ ìœ ì‚¬ ì§€í‘œ
                if total_intra_distance > 0:
                    separation_score = avg_inter / (total_intra_distance / len(clusters))
                    print(f"    ğŸ¯ ë¶„ë¦¬ë„ ì ìˆ˜: {separation_score:.2f} (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)")
    
    def generate_optimized_routes(self) -> Dict:
        """ìµœì í™”ëœ ê²½ë¡œ ìƒì„± - ë³‘ë ¬ ì²˜ë¦¬ ì ìš©"""
        print("\nğŸš€ ë°°ì°¨ ìµœì í™” ì‹œì‘...")
        
        # TCë³„ ë°°ì†¡ì§€ ì¶”ì¶œ
        tc_deliveries = self.extract_delivery_points_by_tc()
        
        optimized_data = {
            'multi_depot': True,
            'depots': self.data['depots'],
            'routes': [],
            'stats': {},
            'tc_stats': [],
            'optimization_info': {
                'algorithm': 'DBSCAN + Balanced K-means (Optimized)',
                'improvements': [
                    'ì§€ì—­ ê¸°ë°˜ ìì—°ìŠ¤ëŸ¬ìš´ í´ëŸ¬ìŠ¤í„°ë§',
                    'í´ëŸ¬ìŠ¤í„° í¬ê¸° ê· í˜• ì¡°ì •',
                    'ì¤‘ë³µ ì˜ì—­ ìµœì†Œí™”',
                    'íš¨ìœ¨ì„± ì§€í‘œ ìµœì í™”',
                    'ë³‘ë ¬ ì²˜ë¦¬ ë° ìºì‹± ìµœì í™”'
                ]
            }
        }
        
        total_routes = []
        total_distance = 0
        total_time = 0
        total_points = 0
        
        # TCë³„ ì²˜ë¦¬ë¥¼ ë³‘ë ¬í™”
        tc_tasks = []
        for tc_stat in self.tc_stats:
            tc_id = tc_stat['tc_id']
            if tc_id in tc_deliveries:
                tc_tasks.append((tc_stat, tc_deliveries[tc_id]))
        
        # ë³‘ë ¬ ì²˜ë¦¬ë¡œ TCë³„ ìµœì í™”
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_tc = {executor.submit(self.process_tc_optimization, task): task for task in tc_tasks}
            
            for future in future_to_tc:
                try:
                    tc_result = future.result()
                    if tc_result:
                        total_routes.extend(tc_result['routes'])
                        total_distance += tc_result['distance']
                        total_time += tc_result['time']
                        total_points += tc_result['points']
                        optimized_data['tc_stats'].append(tc_result['tc_stat'])
                except Exception as e:
                    print(f"âŒ TC ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        
        # ì „ì²´ í†µê³„
        optimized_data['routes'] = total_routes
        optimized_data['stats'] = {
            'total_points': total_points,
            'total_vehicles': len(total_routes),
            'total_distance': total_distance,
            'total_time': total_time,
            'avg_distance_per_vehicle': total_distance / len(total_routes) if total_routes else 0,
            'avg_time_per_vehicle': total_time / len(total_routes) if total_routes else 0,
            'tc_count': len(self.tc_stats),
            'optimization_applied': True,
            'time_efficiency': (total_points / total_time) if total_time > 0 else 0
        }
        
        return optimized_data
    
    def process_tc_optimization(self, task_data) -> Dict:
        """TCë³„ ìµœì í™” ì²˜ë¦¬ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
        tc_stat, deliveries = task_data
        tc_id = tc_stat['tc_id']
        tc_name = tc_stat['tc_name']
        vehicle_count = tc_stat['vehicles']
        
        print(f"\nğŸ¢ {tc_name}: {len(deliveries)}ê°œ ë°°ì†¡ì§€ â†’ {vehicle_count}ëŒ€ ì°¨ëŸ‰")
        
        # ìµœì í™”ëœ í´ëŸ¬ìŠ¤í„°ë§
        clusters = self.optimize_tc_clustering(tc_id, deliveries, vehicle_count)
        
        # ê° í´ëŸ¬ìŠ¤í„°ë¥¼ ì°¨ëŸ‰ì— í• ë‹¹
        tc_routes = []
        tc_distance = 0
        tc_time = 0
        
        for i, cluster in enumerate(clusters):
            if not cluster:
                continue
            
            vehicle_id = len(tc_routes) + 1
            tc_vehicle_number = i + 1
            
            # ê¸°ì¡´ ë°ì´í„°ì—ì„œ í•´ë‹¹ ì°¨ëŸ‰ ì •ë³´ ì°¾ê¸°
            original_route = None
            for route in self.routes:
                if route.get('depot_id') == tc_id:
                    original_route = route
                    break
            
            if original_route:
                # ê°„ì†Œí™”ëœ ê±°ë¦¬/ì‹œê°„ ì¶”ì •
                estimated_distance = len(cluster) * 2.5  # ë°°ì†¡ì§€ë‹¹ í‰ê·  2.5km ì¶”ì •
                estimated_time = len(cluster) * 15  # ë°°ì†¡ì§€ë‹¹ í‰ê·  15ë¶„ ì¶”ì •
                
                route_data = {
                    'vehicle_id': vehicle_id,
                    'vehicle_name': f'{tc_vehicle_number}í˜¸ì°¨',
                    'vehicle_type': original_route.get('vehicle_type', 'TRUCK_1TON'),
                    'depot_id': tc_id,
                    'depot_name': tc_name,
                    'delivery_count': len(cluster),
                    'distance': estimated_distance,
                    'time': estimated_time,
                    'coordinates': [
                        {
                            'id': tc_id,
                            'label': tc_name,
                            'lat': original_route['coordinates'][0]['lat'],
                            'lng': original_route['coordinates'][0]['lng'],
                            'type': 'depot'
                        }
                    ]
                }
                
                # í´ëŸ¬ìŠ¤í„°ì˜ ë°°ì†¡ì§€ë“¤ ì¶”ê°€
                for j, delivery in enumerate(cluster):
                    route_data['coordinates'].append({
                        'id': f"{tc_id}_delivery_{vehicle_id}_{j+1}",
                        'label': delivery['label'],
                        'lat': delivery['lat'],
                        'lng': delivery['lng'],
                        'type': 'delivery',
                        'sequence': j + 1
                    })
                
                # ë‹¤ì‹œ depotìœ¼ë¡œ ëŒì•„ê°€ê¸°
                route_data['coordinates'].append(route_data['coordinates'][0])
                
                tc_routes.append(route_data)
                tc_distance += estimated_distance
                tc_time += estimated_time
        
        print(f"  âœ… ìµœì í™” ì™„ë£Œ: {len(tc_routes)}ëŒ€ ì°¨ëŸ‰, ì˜ˆìƒ ê±°ë¦¬ {tc_distance:.1f}km")
        
        return {
            'routes': tc_routes,
            'distance': tc_distance,
            'time': tc_time,
            'points': len(deliveries),
            'tc_stat': {
                'tc_id': tc_id,
                'tc_name': tc_name,
                'delivery_points': len(deliveries),
                'vehicles': len(tc_routes),
                'total_distance': tc_distance,
                'total_time': tc_time
            }
        }

    def save_optimized_routes(self, output_file: str) -> None:
        """ìµœì í™”ëœ ê²½ë¡œë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        optimized_data = self.generate_optimized_routes()
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(optimized_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ìµœì í™”ëœ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_file}")

def optimize_routes_with_clustering(data: Dict) -> Dict:
    """
    í´ëŸ¬ìŠ¤í„°ë§ ìµœì í™”ë¥¼ ì ìš©í•œ ê²½ë¡œ ìƒì„± (ì™¸ë¶€ í˜¸ì¶œìš©)
    
    Args:
        data: ê¸°ì¡´ ê²½ë¡œ ë°ì´í„°
        
    Returns:
        í´ëŸ¬ìŠ¤í„°ë§ ìµœì í™”ëœ ê²½ë¡œ ë°ì´í„°
    """
    optimizer = ClusterOptimizer(data=data)
    return optimizer.generate_optimized_routes()

# ê¸°ì¡´ main í•¨ìˆ˜ëŠ” í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ìœ ì§€
def main():
    """í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ í•¨ìˆ˜"""
    input_file = Path("../../data/extracted_coordinates.json")  # ìƒëŒ€ ê²½ë¡œ ìˆ˜ì •
    output_file = Path("../../data/cluster_optimized_coordinates.json")
    
    if not input_file.exists():
        print(f"âŒ ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_file}")
        return
    
    optimizer = ClusterOptimizer(data_file=str(input_file))
    optimizer.save_optimized_routes(str(output_file))
    
    print(f"\nğŸ‰ í´ëŸ¬ìŠ¤í„°ë§ ìµœì í™” ì™„ë£Œ!")
    print(f"ğŸ“ ìµœì í™”ëœ ë°ì´í„°: {output_file}")

if __name__ == "__main__":
    main() 